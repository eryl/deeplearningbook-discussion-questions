\documentclass[a4paper]{report}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}

\setcounter{chapter}{9}

\renewcommand{\vec}[1]{{\boldsymbol{#1}}}
\newcommand{\tvec}[2]{{\vec{#1}^{(#2)}}}
\newcommand{\svec}[2]{{\vec{#1}_{#2}}}
\newcommand{\stvec}[3]{{\vec{#1}_{#2}^{(#3)}}}

\begin{document}
\chapter{Questions for Sequence Modelling: Recurrent and Recursive Nets}

\section{Questions for Unfolding Computational Graphs}

  \subsection{} The authors state that \textit{"Much as almost any function can be considered a feed forward neural network, essentially any function involving recurrence can be considered a recurrent neural network."} Do you agree with this statement?
  \subsection{} When predicting the future from the past, an RNN learns to use its hidden state to summarize the history of observations. This is in general a necessarily lossy summary, since the history sequence can be any length and the hidden state is finite. Give an example when this is irrelevant for the prediction task. Give an example when this make a big Difference.
    \subsection{}  The unfolded view of a neural network illustrate how information flows forward in time. Is this necessarily the same time direction as in the data (i.e. from an observed time-series)? If yes, explain why. If no, give a counter-example.
    \subsection{}  If we were to imagine neural networks as implementing algorithms using a \href{https://en.wikipedia.org/wiki/Control-flow_graph}{Control-Flow Graph}, how would you characterize the difference between the GFG implemented by a Feed forward Neural Networks compared to a Recurrent Neural Networks?

\section{Questions for Recurrent Neural Networks}
{} The authors assume hyperbolic tangent units, not ReLU which in earlier chapters has been the most common one. Why do you think that is?

\subsection{} What is the fundamental difference between the two deep RNNs described by the equations \ref{IIR} and \ref{FIR} below?

  \begin{equation}
    \label{IIR}
\begin{gathered}
  \stvec{h}{1}{t} = \tanh(\vec{b}_1 + W_1 \stvec{h}{1}{t-1} + U_1\tvec{x}{t})\\
  \stvec{h}{2}{t} = \tanh(\vec{b}_2 + W_2 \stvec{h}{2}{t-1} + U_2\stvec{h}{1}{t})\\
  \vdots \\
    \stvec{h}{l}{t} = \tanh(\vec{b}_l + W_l \stvec{h}{l}{t-1} + U_l\stvec{h}{l-1}{t})\\
    \tvec{o}{t} = c + V\stvec{h}{l}{t}
\end{gathered}
\end{equation}

  \vspace{10mm}

  \begin{equation}
    \label{FIR}
\begin{gathered}
  \stvec{h}{1}{t} = \tanh(\vec{b}_1 + W_1 \tvec{x}{t-1} + U_1\tvec{x}{t})\\
  \stvec{h}{2}{t} = \tanh(\vec{b}_2 + W_2 \stvec{h}{1}{t-1} + U_2\stvec{h}{1}{t})\\
  \vdots \\
    \stvec{h}{l}{t} = \tanh(\vec{b}_l + W_l \stvec{h}{l-1}{t-1} + U_l\stvec{h}{l-1}{t})\\
    \tvec{o}{t} = c + V\stvec{h}{l}{t}
\end{gathered}
\end{equation}

  \subsection{} Below (\ref{NLL}) is a reproduction of equation 10.14 from the book. Under what independence assumptions is it modelling the joint probability $P(y^{(1)}, y^{(2)}, \ldots, y^{(1)})$?

  \begin{equation}
    \label{NLL}
    \begin{gathered}
      -\sum_t \log p_{\text{model}}(y^{(t)} | \{\tvec{x}{1}, \tvec{x}{2}, \ldots, \tvec{x}{t}\}
\end{gathered}
\end{equation}

  \subsection{}
  A common setup for training RNNs on sequence data (text and time-series) is to use next-step prediction (see equation \ref{NSP} below). In what way (if any) is this different from the network described by figure 10.4 of the book, and is it an application of teacher forcing?

  \begin{equation}
    \label{NSP}
    \begin{gathered}
      \tvec{h}{t} = \tanh(\vec{b} + W \tvec{h}{t-1} + U\tvec{x}{t})\\
    \tvec{o}{t} = \vec{c} + V\tvec{h}{t}\\
    \tvec{y}{t} = \text{softmax}(\tvec{o}{t})\\
    L^{(t)} = -\sum_{c=1}^C \vec{1}_{\tvec{x}{t+1}} \log p(y_c^{(t)})
\end{gathered}
\end{equation}
  (Some notes on the loss, in this case we assume that $\vec{x}$ is a vector encoding of discrete values from the set $C$, if the next $\vec{x}$ is $c$, the indicator function
  $\vec{1}_{\tvec{x}{t+1}}$ takes the value 1, otherwise 0. This means that the loss
    will only be for the value of $\tvec{y} {t}$ which corresponds to the
    probability of the next value of $\vec{x}$)

    \subsection{}
    The Back-Propgation Through Time algorithm described can train RNNs for arbitrarily long sequences, but in practice it is fundamentally limited by computational resources. What computational resource is the limiting factor, and why is it so?

    \subsection{}
    Examine equation 10.21 of the book. Start at $t=1$ and expand the expression forwards in time for a few steps (lets say 3). Can you make any interesting observations about the power of the different terms and factors of the expanded expression?

    \subsection{}
    A common way of modeling a joint distribution of a sequence is given in equation 10.31. In this case, we use the product rule of probability to factor the conditionals in a forwards-prediction. Imagine instead that we would like model the conditional distribution for arbitrary elments of a sequence, see equation below. How could you model this with one or more RNNs?

    {
    \[
    \begin{gathered}
      P(\mathbb{Y}) = P(\tvec{y}{1}, \ldots, \tvec{y}{\tau}) = \\
 P(\tvec{y}{t} | \tvec{y}{\tau}, \ldots, \tvec{y}{t+1}, \tvec{y}{t-1}, \ldots, \tvec{y}{1}) \prod_{i=1}^{t-1} P(\tvec{y}{i} | \tvec{y}{i-1}, \ldots, \tvec{y}{1})\prod_{i=t+1}^{\tau} P(\tvec{y}{i} | \tvec{y}{i-1}, \ldots, \tvec{y}{t+1})
\end{gathered}
    \]}

    \subsection{}
    To draw samples from an RNN, the book suggests three different ways to control how many time-steps the sampling is done over. Explain how the Bernoulli and length-prediction methods are actually very similar.

    \subsection{}
    The book claims that the length-prediction control method needs to also have the predicted length as input. Explain how this is similar to teacher forcing.

    \subsection{}
    The book claims that the length-prediction control method needs to also have the predicted length as input. Assuming the RNN which uses this control method has hidden-to-hidden recurrence, is it in principle necessary to have the predicted length as an extra input?

    \subsection{}
    The book gives two main strategies for conditioning an RNN on some single vector input $\vec{x}$ (itemized list on page 391): As an extra input at each time step or as the initial state $\tvec{h}{0}$. For each of these strategies, explain their drawbacks.

    \subsection{}
    Figure 10.9 in the book shows an example of an RNN which takes a fixed length vector as input at each time sequence. Can you come up with some other example than image captioning where this would be useful?

    \subsection{}
    Explain why the RNN in equation 10.8 correspond to the conditional independence assumption in equation 10.35.

    \section{Bidirectional RNNs}
    \subsection{}
    A bidirectional RNN process a sequence in two intuitive orders, forwards and backwards. Could you use an RNN to process a sequence in some other order? When would you want to do that?
    \subsection{}
    In what sense are RNNs applied to images more expensive than CNNs?

    \section{Encoder-Decoder Sequence-to-Sequence Architectures}
    \subsection{}
    We could train a sequence-to-sequence model with an auto-encoder objective, where the goal is to
    reproduce the input (essentially copy the context sequence through the encoder bottleneck).
    If the sequences are english sentences, what do you think the intermediate
    representation (the context C) would learn to capture?
    \subsection{}
    It has been noted (e.g. Sutskever et al. 2014) that the performance of sequence-to-sequence architectures improves if we reverse the target sequence from the true order (so for translation from english to french, the target sentence in french has its words in reverse order). Why do you think this is the case?
    \section{Deep Recurrent Networks}
    \subsection{}
    Consider the deep RNN depicted in figure 10.13a. Would it still be a \emph{deep} recurrent network if there where also recurrent connections going from $z$ to $h$?
    \subsection{}
    Consider figure 10.13b. What would happen if you changed where the delay tap was, so instead of being after the extra hidden recurrent layer, it was before it?

    \section{Recursive Neural Networks}
    \subsection{}
    What is the difference between a recursive neural network and a convolutional neural network?

    \section{The Challenge of Long-Term Dependencies}
    \subsection{}
    In the case of a linear recurrent network (without nonlinear activation functions), the authors explain how by looking at the largest eigenvalue of the recurrent matrix we can understand how a signal explodes or vanishes after repeated multiplications with the matrix. For a linear recurrent network this behaviour is the same in the forwards and backwards (computing gradients) direction. For a network with a nonlinear activation function (e.g. tanh), the forwards and backwards path behaves very differently, explain how and why.

    \section{Echo State Networks}


\end{document}
